---
title: "Give me some credit - Code compilation"
author: "Emma Godfrey"
date: "10/26/2020"
output: pdf_document
---
The code outline is as follows: 
1. data cleaning, outlier detection, and imputation
2. Balancing the dataset via minority oversampling and synthetic minority data generation
3. XGBoost model generation and testing

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
cs.training <- read.csv("~/Downloads/cs-training.csv")
cs.test <- read.csv("~/Downloads/cs-test.csv")

library("tidyverse") 
library("MASS")
library("e1071")
library("class")
library("xgboost")
library("outliers")
library('EnvStats')
library("ggstatsplot")
library('randomForest')
library('missForest')
library('gapminder')
library('mice')
library('readr')
library('sm')
library("ROSE")
library('rpart')
library('rpart.plot')
library("DiagrammeR")
```


```{r cleaning and imputation}
# create indicator variables to show whether variable was imputed or not
cs.training <- cs.training %>%
  mutate(missing_income = ifelse(is.na(MonthlyIncome), 1, 0),
         missing_dependents = ifelse(is.na(NumberOfDependents), 1, 0),
         ninetyeight = ifelse(NumberOfTime30.59DaysPastDueNotWorse > 80, 1, 0),
         debt.more.than.25000 = ifelse(DebtRatio>25000, 1,0),
         creditlines.more.than.25 = ifelse(NumberOfOpenCreditLinesAndLoans > 25, 1, 0),
         realestate.more.than.15 = ifelse(NumberRealEstateLoansOrLines > 15, 1, 0), 
         revolving.more.than.1.5 = ifelse(RevolvingUtilizationOfUnsecuredLines > 1.5, 1, 0)
         )

# create indicator variables for the test data to show whether imputed or not 
# cutoffs were decided via EDA and outlier detection
cs.test <- cs.test %>% 
   mutate(missing_income = ifelse(is.na(MonthlyIncome), 1, 0),
         missing_dependents = ifelse(is.na(NumberOfDependents), 1, 0),
         ninetyeight = ifelse(NumberOfTime30.59DaysPastDueNotWorse > 80, 1, 0),
         debt.more.than.25000 = ifelse(DebtRatio>25000, 1,0),
         creditlines.more.than.25 = ifelse(NumberOfOpenCreditLinesAndLoans > 25, 1, 0),
         realestate.more.than.15 = ifelse(NumberRealEstateLoansOrLines > 15, 1, 0), 
         revolving.more.than.1.5 = ifelse(RevolvingUtilizationOfUnsecuredLines > 1.5, 1, 0)
         )

# change outliers to NAs which we will impute later
cs.training.impute <- cs.training %>%
  mutate(RevolvingUtilizationOfUnsecuredLines = ifelse(RevolvingUtilizationOfUnsecuredLines>1.5, NA, RevolvingUtilizationOfUnsecuredLines),
         NumberOfTime30.59DaysPastDueNotWorse = ifelse(NumberOfTime30.59DaysPastDueNotWorse>30, NA, NumberOfTime30.59DaysPastDueNotWorse), 
         NumberOfTime60.89DaysPastDueNotWorse = ifelse(NumberOfTime60.89DaysPastDueNotWorse>30, NA, NumberOfTime60.89DaysPastDueNotWorse),
         NumberOfTimes90DaysLate = ifelse(NumberOfTimes90DaysLate>30, NA, NumberOfTimes90DaysLate),
         DebtRatio = ifelse(DebtRatio > 25000, NA, DebtRatio),
         MonthlyIncome = ifelse(MonthlyIncome > 50000 | MonthlyIncome <600, NA, MonthlyIncome),
         NumberOfOpenCreditLinesAndLoans = ifelse(NumberOfOpenCreditLinesAndLoans >25, NA, NumberOfOpenCreditLinesAndLoans),
         NumberRealEstateLoansOrLines = ifelse(NumberRealEstateLoansOrLines >10, NA, NumberRealEstateLoansOrLines),
         NumberOfDependents = ifelse(NumberOfDependents > 7, NA, NumberOfDependents)
         )

# change outliers to NA which we will later impute
cs.test.impute <- cs.test %>% 
    mutate(RevolvingUtilizationOfUnsecuredLines = ifelse(RevolvingUtilizationOfUnsecuredLines>1.5, NA, RevolvingUtilizationOfUnsecuredLines),
         NumberOfTime30.59DaysPastDueNotWorse = ifelse(NumberOfTime30.59DaysPastDueNotWorse>30, NA, NumberOfTime30.59DaysPastDueNotWorse), 
         NumberOfTime60.89DaysPastDueNotWorse = ifelse(NumberOfTime60.89DaysPastDueNotWorse>30, NA, NumberOfTime60.89DaysPastDueNotWorse),
         NumberOfTimes90DaysLate = ifelse(NumberOfTimes90DaysLate>30, NA, NumberOfTimes90DaysLate),
         DebtRatio = ifelse(DebtRatio > 25000, NA, DebtRatio),
         MonthlyIncome = ifelse(MonthlyIncome > 50000 | MonthlyIncome <600, NA, MonthlyIncome),
         NumberOfOpenCreditLinesAndLoans = ifelse(NumberOfOpenCreditLinesAndLoans >25, NA, NumberOfOpenCreditLinesAndLoans),
         NumberRealEstateLoansOrLines = ifelse(NumberRealEstateLoansOrLines >10, NA, NumberRealEstateLoansOrLines),
         NumberOfDependents = ifelse(NumberOfDependents > 7, NA, NumberOfDependents)
         )

# Overall indicator variable to show whether record was imputed 
cs.training.impute <- cs.training.impute %>% 
  mutate(imputed = ifelse(complete.cases(cs.training.impute), 0, 1))

cs.test.impute <- cs.test.impute %>% 
  mutate(imputed = ifelse(complete.cases(cs.test.impute), 0, 1))


# looking at distribution of classes to determine MAR or MNAR
imputed.yes <- cs.training.impute %>%
  filter(imputed == 1) 
imputed.no <- cs.training.impute %>% 
  filter(imputed == 0)

# MICE implementation -- takes ~1.5 hrs to run 
cs.training1.impute <- mice(cs.training.impute, m=5, maxit=25, meth='pmm')
complete.data <- complete(cs.training1.impute, 1)
# z-scale the data
complete.data[,3:12] <- apply(complete.data[,3:12],2, scale)


# MICE implementation for test data
cs.test1.impute <- mice(cs.test.impute, m=5, maxit=25, meth='pmm')
# construct fully imputed dataset
complete.test.imputed <- complete(cs.test1.impute, 1)
# z-scale the imputed test dataset
complete.test.imputed[,3:12] <- apply(complete.test.imputed[,3:12],2,scale)


# create imputed testing and training data
random.sample <- sample(nrow(complete.data), 0.8*nrow(complete.data), replace=FALSE)
complete.training <- complete.data[random.sample,]
complete.testing <- complete.data[-random.sample,]

# check distribution of income post-imputation
ggplot(complete.data, aes(x= MonthlyIncome)) + 
  geom_histogram() +
  ggtitle("Distribution of Scaled Monthly Income After MICE Implementation")
```

```{r balancing the dataset}

par(mfrow=c(1,3))

# synthetically generating minority data 
data.rose <- ROSE(SeriousDlqin2yrs ~., data = complete.training, seed=1)$data
# fit a classification tree to synthetically generated dataset
treeimb <- rpart(data.rose$SeriousDlqin2yrs ~ ., data = data.rose)
treeplot <- rpart.plot(treeimb, type =1, main = "SMOTE Balanced Data Classification Tree Model")
pred.treeimb <- predict(treeimb, newdata = complete.testing)
# check the ROC curve for learner prediction power
roc.curve(complete.testing$SeriousDlqin2yrs, pred.treeimb, n.thresholds = 20)
accuracy.meas(complete.testing$SeriousDlqin2yrs, pred.treeimb)

# over sampling data and building classification tree 
data.balanced.over <- ovun.sample(SeriousDlqin2yrs ~ ., data = complete.training, method = "over",N = 150000)$data
tree.over <- rpart(data.balanced.over$SeriousDlqin2yrs ~., data = data.balanced.over)
rpart.plot(tree.over, type = 1, main = "Oversampled Balanced Data Classification Tree Model")
pred.tree.over <- predict(tree.over, newdata = complete.testing)
roc.curve(complete.testing$SeriousDlqin2yrs, pred.tree.over)

# unbalanced data, normal classification tree and plot for ROC curve 
tree.normal <- rpart(complete.training$SeriousDlqin2yrs ~., data = complete.training)
pred.tree.normal <- predict(tree.normal, newdata = complete.testing)
rpart.plot(tree.normal, type = 1, main = "Imbalanced Data Classification Tree Model")
roc.curve(complete.testing$SeriousDlqin2yrs, pred.tree.normal)


# code to plot the importance plots for each classification tree 
df <- data.frame(imp = tree.normal$variable.importance)
df2 <- df %>% 
  tibble::rownames_to_column() %>% 
  dplyr::rename("variable" = rowname) %>% 
  dplyr::arrange(imp) %>%
  dplyr::mutate(variable = forcats::fct_inorder(variable))
plot1 <- ggplot2::ggplot(df2) +
  geom_col(aes(x = variable, y = imp),
           col = "black", show.legend = F) +
  coord_flip() +
  scale_fill_grey() +
  theme_bw() + 
  ggtitle("Importance Plot for Classification Tree (Imbalanced)")

df3 <- data.frame(imp = tree.over$variable.importance)
df4 <- df %>% 
  tibble::rownames_to_column() %>% 
  dplyr::rename("variable" = rowname) %>% 
  dplyr::arrange(imp) %>%
  dplyr::mutate(variable = forcats::fct_inorder(variable))
plot2 <- ggplot2::ggplot(df4) +
  geom_col(aes(x = variable, y = imp),
           col = "black", show.legend = F) +
  coord_flip() +
  scale_fill_grey() +
  theme_bw() + 
  ggtitle("Importance Plot for Classification Tree (Oversampling)")

df5 <- data.frame(imp = treeimb$variable.importance)
df6 <- df %>% 
  tibble::rownames_to_column() %>% 
  dplyr::rename("variable" = rowname) %>% 
  dplyr::arrange(imp) %>%
  dplyr::mutate(variable = forcats::fct_inorder(variable))
plot3 <- ggplot2::ggplot(df6) +
  geom_col(aes(x = variable, y = imp),
           col = "black", show.legend = F) +
  coord_flip() +
  scale_fill_grey() +
  theme_bw() + 
  ggtitle("Importance Plot for Classification Tree (SMOTE)")

```

 

XGBoost Code. 
```{r unbalanced xgboost model}
# create unlabeled and only labels training data
complete.training.nolables <- complete.training[,-2]
complete.training.labels <- complete.training[,2]
complete.testing.nolabels <- complete.testing[,-2]
complete.testing.labels <- complete.testing[,2]

# XGBoost requires the data to be in specific format
complete.boost.train <- xgb.DMatrix(data=as.matrix(complete.training.nolables), label=as.matrix(complete.training.labels))

complete.boost.test <- xgb.DMatrix(data= as.matrix(complete.testing.nolabels), label=as.matrix(complete.testing.labels))

# to be used in balancing parameter in XGBoost 
positive_cases <- sum(complete.training.labels == 1)
negative_cases <- sum(complete.training.labels == 0)

# parameters for XGBoost model
params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)

# cv for nrounds -- best iteration = 16
xgbcv <- xgb.cv( params = params, data = complete.boost.train, nrounds = 100, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)

# run the full XGBoost model with cv nrounds 
xgb.imputed.model <- xgboost(params = params, 
                             data= complete.boost.train,
                            nround = 16,
                            scale_pos_weight = negative_cases/positive_cases)
# We see minimal improvement from round 1 to round 2 
# test our model on our training data
testing <- predict(xgb.imputed.model, complete.boost.test)

# plot ROC curve for XGBoost predictions 
roc.curve(complete.testing$SeriousDlqin2yrs, testing, main = "Imbalanced Dataset: AUC = 0.861")

```

```{r}
# looking inside the boosted tree 
xgb.plot.tree(model = xgb.model, trees = 0:1)

# constructing an importance plot for imblaanced XGB model
importance1 <- xgb.importance(model = xgb.imputed.model)
importance.plot <- xgb.plot.importance(importance1[1:8], main = "Imbalanced Dataset XGBoost Variable Importance", left_margin = 15, xlab = "Variable Importance" )
```

```{r more visualizations}

# graph of predictions and truth at each point
plot(complete.testing.nolabels$age, complete.testing.nolabels$DebtRatio, col = complete.testing.labels +1)
points(complete.testing.nolabels$age, complete.testing.nolabels$DebtRatio, col = predictions+1, pch=21, cex=.5)
```

```{r balanced XGBoost model}
# create unlabeled and label datset
data.balanced.over.nolabels<- data.balanced.over[,-2]
data.balanced.over.labels <- data.balanced.over[,2]

# transform into matrix
data.balanced.over.boost.train <- xgb.DMatrix(data=as.matrix(data.balanced.over.nolabels), label=as.matrix(data.balanced.over.labels))

# parameter for model
params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)

# cv for nrounds -- best iteration = 15
xgbcv.over <- xgb.cv( params = params, data = data.balanced.over.boost.train, nrounds = 400, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)
xgbcv.over$best_iteration

# run full model with cv best nrounds
xgb.imputed.model2 <- xgboost(params = params, 
                             data= data.balanced.over.boost.train,
                            nround = 15)

# test our model on our training data
tests <- predict(xgb.imputed.model2, complete.boost.test)

# plot roc curveand importance plots
roc.curve(complete.testing$SeriousDlqin2yrs, tests,main = "Balanced Dataset: AUC = 0.863")
importance2<- xgb.importance(model = xgb.imputed.model2)
importance.plot2 <- xgb.plot.importance(importance[1:8], main = "Balanced Dataset XGBoost Variable Importance", left_margin = 15, xlab = "Variable Importance" )

# prepare test data for prediction
complete.test.imputed.nolabels <- complete.test.imputed[,-2]
complete.test.imputed.labels <- complete.test.imputed[,2]

# put test data into matrix form 
data.balanced.over.boost.test <- xgb.DMatrix(data=as.matrix(complete.test.imputed.nolabels), label=as.matrix(complete.test.imputed.labels))

# use model to predict probabilities to be submitted to kaggle 
probs <- predict(xgb.imputed.model2, data.balanced.over.boost.test)

# preparing final submission 
sampleEntry <- read.csv("~/Downloads/sampleEntry.csv")
sampleEntry[,2] <- probs
write.csv(sampleEntry, 'submission.csv', row.names = FALSE)

```


